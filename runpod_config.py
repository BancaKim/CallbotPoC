"""
Runpod íŠ¹í™” ì„¤ì • íŒŒì¼
GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ìµœì í™”ëœ ì„¤ì •
"""

import os

class RunpodConfig:
    """Runpod í™˜ê²½ì„ ìœ„í•œ ìµœì í™”ëœ ì„¤ì •"""
    
    # GPU ë©”ëª¨ë¦¬ ì„¤ì • (í˜„ì¬ í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
    GPU_MEMORY_UTILIZATION = float(os.getenv("GPU_MEMORY_UTILIZATION", "0.7"))  # 0.8ì—ì„œ 0.7ë¡œ ê°ì†Œ
    
    # ëª¨ë¸ ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½í˜•)
    WHISPER_MODEL = os.getenv("WHISPER_MODEL", "base")  # large-v3 ëŒ€ì‹  base ì‚¬ìš©
    LLM_MODEL = os.getenv("LLM_MODEL", "Qwen/Qwen2-7B-Instruct")
    
    # ëŒ€ì•ˆ LLM ëª¨ë¸ë“¤ (ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ì‚¬ìš©)
    ALTERNATIVE_MODELS = [
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-small", 
        "distilgpt2"
    ]
    
    # RAG ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))  # 1000ì—ì„œ 800ìœ¼ë¡œ ê°ì†Œ
    OVERLAP_SIZE = int(os.getenv("OVERLAP_SIZE", "150"))  # 200ì—ì„œ 150ìœ¼ë¡œ ê°ì†Œ
    MAX_SEARCH_RESULTS = int(os.getenv("MAX_SEARCH_RESULTS", "3"))  # 5ì—ì„œ 3ìœ¼ë¡œ ê°ì†Œ
    
    # vLLM ì„¤ì •
    MAX_MODEL_LEN = int(os.getenv("MAX_MODEL_LEN", "4096"))  # 8192ì—ì„œ 4096ìœ¼ë¡œ ê°ì†Œ
    
    @classmethod
    def get_optimal_gpu_memory(cls):
        """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœì— ë”°ë¥¸ ìµœì  ì‚¬ìš©ë¥  ë°˜í™˜"""
        try:
            import torch
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                if total_memory > 40:  # 40GB ì´ìƒ
                    return 0.75
                elif total_memory > 20:  # 20-40GB
                    return 0.7  
                else:  # 20GB ë¯¸ë§Œ
                    return 0.6
        except:
            pass
        return cls.GPU_MEMORY_UTILIZATION
    
    @classmethod
    def should_use_vllm(cls):
        """GPU ë©”ëª¨ë¦¬ ìƒíƒœì— ë”°ë¼ vLLM ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        try:
            import torch
            if torch.cuda.is_available():
                available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                return available_memory >= 16  # 16GB ì´ìƒì¼ ë•Œë§Œ vLLM ì‚¬ìš©
        except:
            pass
        return False
    
    @classmethod
    def get_fallback_settings(cls):
        """ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ëŒ€ì•ˆ ì„¤ì •"""
        return {
            "whisper_model": "tiny",  # ê°€ì¥ ì‘ì€ Whisper ëª¨ë¸
            "gpu_memory_utilization": 0.5,
            "max_model_len": 2048,
            "chunk_size": 500,
            "overlap_size": 100,
            "use_vllm": False  # Transformersë§Œ ì‚¬ìš©
        }

# í™˜ê²½ë³„ ì„¤ì • ë¡œë“œ
def load_runpod_config():
    """Runpod í™˜ê²½ ì„¤ì • ë¡œë“œ"""
    config = RunpodConfig()
    
    print(f"ğŸ”§ Runpod ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {config.get_optimal_gpu_memory()}")
    print(f"   - Whisper ëª¨ë¸: {config.WHISPER_MODEL}")
    print(f"   - LLM ëª¨ë¸: {config.LLM_MODEL}")
    print(f"   - vLLM ì‚¬ìš©: {config.should_use_vllm()}")
    
    return config 